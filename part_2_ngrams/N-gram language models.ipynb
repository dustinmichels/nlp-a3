{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models (12 + 10 + 10 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ngram:\n",
    "_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a wikipedia dataset:\n",
    "# ! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "# ! unzip wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus reader\n",
    "# this includes: sentence segmentation and word tokenization:\n",
    "wikitext2 = PlaintextCorpusReader(\n",
    "    'wikitext-2-raw',\n",
    "    ['wiki.train.raw', 'wiki.valid.raw', 'wiki.test.raw'],\n",
    ")\n",
    "word_tokenizer = wikitext2._word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split:\n",
    "train = wikitext2.sents('wiki.train.raw')\n",
    "test = wikitext2.sents('wiki.test.raw')\n",
    "\n",
    "# the vocabulary based on the training data:\n",
    "vocab = nltk.lm.Vocabulary([\n",
    "    word\n",
    "    for sent in train\n",
    "    for word in sent\n",
    "], unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n-grams\n",
    "def build_ngrams(sent, n):\n",
    "    # pad both ends for corner-ngrams:\n",
    "    sent = ['<s>']*(n-1) + sent + ['</s>']*(n-1)\n",
    "    # build the ngrams:\n",
    "    return list(ngrams(sent, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to inspect how it works:\n",
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_tokinized = word_tokenizer.tokenize(sample)\n",
    "sample_trigrams = build_ngrams(sample_tokinized, n=3)\n",
    "print('sample_tokinized:')\n",
    "print(sample_tokinized)\n",
    "print('sample_trigrams')\n",
    "print(sample_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compare these two models:\n",
    "models = {\n",
    "    'plain': nltk.lm.MLE, # plain count-based ngrams\n",
    "    'smoothing': nltk.lm.Laplace, # with laplace smoothing\n",
    "    'smoothing+interpolation': nltk.lm.KneserNeyInterpolated, # Modified Kneser & Ney \n",
    "}\n",
    "\n",
    "for lm_name in models:\n",
    "    # build and train the language model:\n",
    "    models[lm_name] = models[lm_name](_N, vocabulary=vocab)\n",
    "\n",
    "    # train on all n-grams (equal or lower order): N, N-1, ..., 1.\n",
    "    for n in tqdm(range(_N, 0, -1), desc=lm_name):\n",
    "        models[lm_name].fit([build_ngrams(sent, n) for sent in train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how fit words:\n",
    "# fit() method builds all kinds of count dictionaries:\n",
    "(\n",
    "    models['plain'].counts[1], # unigrams\n",
    "    models['plain'].counts[2], # bi-grams for conditional count freq (w_{t} | w_{t-1})\n",
    "    models['plain'].counts[3], # tri-grams for conditional count freq (w_{t} | w_{t-2} w_{t-1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example: \n",
    "# Count( word_3='numer'   | word_1 = 'A', word_2 ='large' ) = 4\n",
    "# Count( word_3='variety' | word_1 = 'A', word_2 ='large' ) = 3\n",
    "# ...\n",
    "list(models['plain'].counts[3].items())[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand this:\n",
    "models['plain'].counts[3][('A', 'large')]['number'] / sum(models['plain'].counts[3][('A', 'large')].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].score('number', ('A', 'large'))\n",
    "# more details in chapter 3 equation 3.12.\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the plain model for random language generation:\n",
    "models['plain'].generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect log probabilities:\n",
    "models['plain'].logscore('mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\")\n",
    "    for n in range(1, _N+1):\n",
    "        print(f\"{n}-gram\", models[model_name].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why these models have `<UNK>` token? What is the log-probability of <UNK> in three models? (3pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models have `<UNK>` so that \n",
    "\n",
    "plain: -inf, smoothing: -21.03873951979207, smoothing + interpolation -16.21348398616622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing+interpolation'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why plain count-based MLE model fails to produce perplexities? What are the possible solutions for it? (3pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Show with an example why Laplace smoothing can produce perplexity for unseen words? (3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English and python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why perplexity of bi-grams are lower than unigrams? (3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English and python)\n",
    "# use models['smoothing'].counts[2] to show how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 1: measure perplexity of conditional trigrams (10pt)\n",
    "\n",
    "The neural network below is based on Bengio et al. (2003). It is trained on moving windows described in chapter 9 figure 9.1 but with trigrams instead of 4-grams.\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "You don't need to train the model. However, a stand alone python code is provided in `bengio_lm.py` if you want to try training it on GPU.\n",
    "\n",
    "Read the code below then report the perplexity of the language model on the sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # neural network framework\n",
    "\n",
    "# encoding the tokens:\n",
    "vocab_list = [word for word, freq in vocab.counts.most_common() if freq > 1]\n",
    "word2idx = {word: idx for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "idx2word = {idx: word for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "\n",
    "def token_encoder(tokens):\n",
    "    if type(tokens) in {list, tuple}:\n",
    "        return [word2idx[token] if token in word2idx else word2idx[vocab.unk_label] for token in tokens]\n",
    "    elif type(tokens) == str:\n",
    "        token = tokens\n",
    "        return word2idx[token] if token in word2idx else word2idx[vocab.unk_label]\n",
    "    print(type(tokens))\n",
    "\n",
    "# moving window language model:\n",
    "# https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "class BengioLM(torch.nn.Module):\n",
    "    def __init__(self, context_size=2, dim=50):\n",
    "        super(BengioLM, self).__init__()\n",
    "        # defining the parameters of the model\n",
    "        self.C = torch.nn.Embedding(len(word2idx), dim) # C\n",
    "        self.Hx_d = torch.nn.Linear(context_size*dim, dim) # d, H\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.Wx_Uf_b = torch.nn.Linear((context_size + 1) * dim, len(word2idx)) # b, U, W\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss_fn = torch.nn.NLLLoss() # negative-log-likelihood loss\n",
    "    \n",
    "    def forward(self, context, target_idx=None):\n",
    "        # function of the model\n",
    "        batch_size = context.shape[0]\n",
    "        x = self.C(context).view(batch_size,-1)\n",
    "        x = torch.cat([x, self.tanh(self.Hx_d(x))], dim=-1)\n",
    "        logprob = self.logsoftmax(self.Wx_Uf_b(x))\n",
    "        \n",
    "        if target_idx is None:\n",
    "            return logprob\n",
    "        else:\n",
    "            loss = self.loss_fn(logprob, target_idx)\n",
    "            return logprob, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is trained with Stochastic Gradient Descent with 10 epochs (skip this):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BengioLM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "sentence_batch_size = 32\n",
    "\n",
    "for e in range(10):\n",
    "    progress_bar = tqdm(range(0, len(train), sentence_batch_size))\n",
    "    for i in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts, targets = zip(*[\n",
    "            (token_encoder(tokens[:-1]), token_encoder(tokens[-1]))\n",
    "            for sent in train[i:i+sentence_batch_size]\n",
    "            for tokens in build_ngrams(sent, 3)\n",
    "        ])\n",
    "\n",
    "        logprob, loss = model.forward(torch.tensor(contexts), torch.tensor(targets))\n",
    "        progress_bar.set_description_str(f\"epoch={e+1},loss={loss.item():.3f}\")\n",
    "        progress_bar.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ran the training code above on GPU and saved it in model.pt.\n",
    "# load the pre-trained language model:\n",
    "device = torch.device('cpu')\n",
    "model = BengioLM()\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can get the conditional log-probabilities of all words in the sentence\n",
    "# P(target | w0, w1):\n",
    "for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sample), n=3):\n",
    "    logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "    print(target, logprobs[0, token_encoder(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code here to report Perplexity of the sample sentence.\n",
    "\n",
    "For more information got to chapter 3, section 3.2.1 and chapter 9, equation 9.12.\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity of a sentence \n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional 2: implement a generate function using pre-trained language model above (10pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
