{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models (12 + 10 + 10 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "# ngram:\n",
    "_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a wikipedia dataset:\n",
    "#! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "#! unzip wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus reader\n",
    "# this includes: sentence segmentation and word tokenization:\n",
    "wikitext2 = PlaintextCorpusReader(\n",
    "    'wikitext-2-raw',\n",
    "    ['wiki.train.raw', 'wiki.valid.raw', 'wiki.test.raw'],\n",
    ")\n",
    "word_tokenizer = wikitext2._word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split:\n",
    "train = wikitext2.sents('wiki.train.raw')\n",
    "test = wikitext2.sents('wiki.test.raw')\n",
    "\n",
    "# the vocabulary based on the training data:\n",
    "vocab = nltk.lm.Vocabulary([\n",
    "    word\n",
    "    for sent in train\n",
    "    for word in sent\n",
    "], unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n-grams\n",
    "def build_ngrams(sent, n):\n",
    "    # pad both ends for corner-ngrams:\n",
    "    sent = ['<s>']*(n-1) + sent + ['</s>']*(n-1)\n",
    "    # build the ngrams:\n",
    "    return list(ngrams(sent, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to inspect how it works:\n",
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_tokinized = word_tokenizer.tokenize(sample)\n",
    "sample_trigrams = build_ngrams(sample_tokinized, n=3)\n",
    "print('sample_tokinized:')\n",
    "print(sample_tokinized)\n",
    "print('\\nsample_trigrams:')\n",
    "print(sample_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compare these two models:\n",
    "models = {\n",
    "    'plain': nltk.lm.MLE, # plain count-based ngrams\n",
    "    'smoothing': nltk.lm.Laplace, # with laplace smoothing\n",
    "    'smoothing+interpolation': nltk.lm.KneserNeyInterpolated, # Modified Kneser & Ney \n",
    "}\n",
    "\n",
    "for lm_name in models:\n",
    "    # build and train the language model:\n",
    "    models[lm_name] = models[lm_name](_N, vocabulary=vocab)\n",
    "\n",
    "    # train on all n-grams (equal or lower order): N, N-1, ..., 1.\n",
    "    for n in tqdm(range(_N, 0, -1), desc=lm_name):\n",
    "        models[lm_name].fit([build_ngrams(sent, n) for sent in train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how fit words:\n",
    "# fit() method builds all kinds of count dictionaries:\n",
    "(\n",
    "    models['plain'].counts[1], # unigrams\n",
    "    models['plain'].counts[2], # bi-grams for conditional count freq (w_{t} | w_{t-1})\n",
    "    models['plain'].counts[3], # tri-grams for conditional count freq (w_{t} | w_{t-2} w_{t-1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example: \n",
    "# Count( word_3='numer'   | word_1 = 'A', word_2 ='large' ) = 4\n",
    "# Count( word_3='variety' | word_1 = 'A', word_2 ='large' ) = 3\n",
    "# ...\n",
    "list(models['plain'].counts[3].items())[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand this:\n",
    "models['plain'].counts[3][('A', 'large')]['number'] / sum(models['plain'].counts[3][('A', 'large')].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].score('number', ('A', 'large'))\n",
    "# more details in chapter 3 equation 3.12.\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the plain model for random language generation:\n",
    "models['plain'].generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect log probabilities:\n",
    "models['plain'].logscore('mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\")\n",
    "    for n in range(1, _N+1):\n",
    "        print(f\"{n}-gram\", models[model_name].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Why these models have `<UNK>` token? What is the log-probability of <UNK> in three models? (3pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models have `<UNK>` tokens so that words that do not occur in the vocabulary more than n times can be counted and assigned a probability.\n",
    "\n",
    "The probability of `<UNK>` in the three models is:\n",
    "\n",
    "- plain: -inf\n",
    "- smoothing: -21.03873951979207\n",
    "- smoothing + interpolation -16.21348398616622\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing+interpolation'].logscore('<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Why plain count-based MLE model fails to produce perplexities? What are the possible solutions for it? (3pt)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-based MLE model fails to produce perplexity since it assigns zero probability to words that appear in an unseen context in the test-set. When the probability is 0 it is not possible to calculate the perplexity. Possible solutions are smoothing and/or backoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Show with an example why Laplace smoothing can produce perplexity for unseen words? (3pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Laplace smoothing adds 1 to all counts, the unseen words will not have a count of zero. That makes it possible to estimate probability and perplexity of sentences with unseen words. \n",
    "\n",
    "In the examples below, you can see that the probability for the unseen word \"Mehdi\" is 0 with the MLE model because the count is zero. Once Laplace smoothing is applied, we are able to calculate the probability even though the word has never been seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].counts[1]['Mehdi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['plain'].score('Mehdi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].counts[1]['Mehdi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].score('Mehdi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Why perplexity of bi-grams are lower than unigrams? (3pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the perplexity, the better the model. The fact that the perplexity of bigrams is lower than unigrams simply means that the bigram model performs better since it uses more context than the unigram model. \n",
    "\n",
    "In the example below, you can see that the probability of \"number\" without considering the context is quite low. (This is the unigram case). Conversely, the probability of \"number\" given \"large\" is higher since you limit the set of potential words that could be based on the context. Considering the context leads to a higher probability which in turn leads to a lower perplexity. This effect continues through at least trigrams as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].counts[1]['number'] / sum(models['smoothing'].counts[1].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].counts[2][('large',)]['number'] / sum(models['smoothing'].counts[2][('large',)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['smoothing'].counts[3][('A', 'large')]['number'] / sum(models['smoothing'].counts[3][('A','large')].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 1: measure perplexity of conditional trigrams (10pt)\n",
    "\n",
    "The neural network below is based on Bengio et al. (2003). It is trained on moving windows described in chapter 9 figure 9.1 but with trigrams instead of 4-grams.\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "You don't need to train the model. However, a stand alone python code is provided in `bengio_lm.py` if you want to try training it on GPU.\n",
    "\n",
    "Read the code below then report the perplexity of the language model on the sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # neural network framework\n",
    "\n",
    "# encoding the tokens:\n",
    "vocab_list = [word for word, freq in vocab.counts.most_common() if freq > 1]\n",
    "word2idx = {word: idx for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "idx2word = {idx: word for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "\n",
    "def token_encoder(tokens):\n",
    "    if type(tokens) in {list, tuple}:\n",
    "        return [word2idx[token] if token in word2idx else word2idx[vocab.unk_label] for token in tokens]\n",
    "    elif type(tokens) == str:\n",
    "        token = tokens\n",
    "        return word2idx[token] if token in word2idx else word2idx[vocab.unk_label]\n",
    "    print(type(tokens))\n",
    "\n",
    "# moving window language model:\n",
    "# https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "class BengioLM(torch.nn.Module):\n",
    "    def __init__(self, context_size=2, dim=50):\n",
    "        super(BengioLM, self).__init__()\n",
    "        # defining the parameters of the model\n",
    "        self.C = torch.nn.Embedding(len(word2idx), dim) # C\n",
    "        self.Hx_d = torch.nn.Linear(context_size*dim, dim) # d, H\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.Wx_Uf_b = torch.nn.Linear((context_size + 1) * dim, len(word2idx)) # b, U, W\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss_fn = torch.nn.NLLLoss() # negative-log-likelihood loss\n",
    "    \n",
    "    def forward(self, context, target_idx=None):\n",
    "        # function of the model\n",
    "        batch_size = context.shape[0]\n",
    "        x = self.C(context).view(batch_size,-1)\n",
    "        x = torch.cat([x, self.tanh(self.Hx_d(x))], dim=-1)\n",
    "        logprob = self.logsoftmax(self.Wx_Uf_b(x))\n",
    "        \n",
    "        if target_idx is None:\n",
    "            return logprob\n",
    "        else:\n",
    "            loss = self.loss_fn(logprob, target_idx)\n",
    "            return logprob, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is trained with Stochastic Gradient Descent with 10 epochs (skip this):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BengioLM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "sentence_batch_size = 32\n",
    "\n",
    "for e in range(10):\n",
    "    progress_bar = tqdm(range(0, len(train), sentence_batch_size))\n",
    "    for i in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts, targets = zip(*[\n",
    "            (token_encoder(tokens[:-1]), token_encoder(tokens[-1]))\n",
    "            for sent in train[i:i+sentence_batch_size]\n",
    "            for tokens in build_ngrams(sent, 3)\n",
    "        ])\n",
    "\n",
    "        logprob, loss = model.forward(torch.tensor(contexts), torch.tensor(targets))\n",
    "        progress_bar.set_description_str(f\"epoch={e+1},loss={loss.item():.3f}\")\n",
    "        progress_bar.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ran the training code above on GPU and saved it in model.pt.\n",
    "# load the pre-trained language model:\n",
    "device = torch.device('cpu')\n",
    "model = BengioLM()\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can get the conditional log-probabilities of all words in the sentence\n",
    "# P(target | w0, w1):\n",
    "for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sample), n=3):\n",
    "    logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "    print(w0, w1, \"-\", target, logprobs[0, token_encoder(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code here to report Perplexity of the sample sentence.\n",
    "\n",
    "For more information got to chapter 3, section 3.2.1 and chapter 9, equation 9.12.\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ PP(W) = n\\sqrt{\\prod_{i = 1}^{n}\\frac{1}{P(w_i|w_{i−1})}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence):\n",
    "    trigram_probs = []\n",
    "    for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sentence), n=3):\n",
    "        logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "        trigram_prob_tensor = logprobs[0, token_encoder(target)]\n",
    "        trigram_prob_log = trigram_prob_tensor.item()\n",
    "        print(f\"log prob of {target}:\", trigram_prob_log)        \n",
    "        trigram_prob = math.e**trigram_prob_log\n",
    "        trigram_probs.append(1/trigram_prob)\n",
    "\n",
    "    trigram_probs = math.prod(trigram_probs)\n",
    "    n = len(sentence)\n",
    "    perplexity = trigram_probs**(1/n)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "pp = calculate_perplexity(sample)\n",
    "print(\"\\nPerplexity:\", pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional 2: implement a generate function using pre-trained language model above (10pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sample, n=10, ignore_unk=False):\n",
    "    \"\"\"\n",
    "    Sample should be string with two words.\n",
    "    \"\"\"\n",
    "    tokens = list(word_tokenizer.tokenize(sample))\n",
    "    \n",
    "    while tokens[-1] != '</s>' and len(tokens) < n: # the criteria to end the loop\n",
    "        logprobs = model.forward(torch.tensor([token_encoder(tokens[-2:])]))\n",
    "\n",
    "        # choose the next word based on` logprobs[0]\n",
    "        indexed_probs = [(idx, val) for idx, val in enumerate(logprobs[0])]\n",
    "        indexed_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        max_idx = indexed_probs[0][0]\n",
    "        next_word = idx2word[max_idx]\n",
    "        \n",
    "        if ignore_unk:\n",
    "            if next_word == '<UNK>':\n",
    "                max_idx = indexed_probs[1][0]\n",
    "                next_word = idx2word[max_idx]\n",
    "\n",
    "        # update tokens list\n",
    "        tokens.append(next_word)\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"A large\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"A large\", n=10, ignore_unk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
