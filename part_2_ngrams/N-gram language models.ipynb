{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models (12 + 10 + 10 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ngram:\n",
    "_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-09 07:40:31--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.104.62\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.104.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4,5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4,50M  3,53MB/s    in 1,3s    \n",
      "\n",
      "2020-12-09 07:40:33 (3,53 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n",
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    }
   ],
   "source": [
    "# Download a wikipedia dataset:\n",
    "! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "! unzip wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus reader\n",
    "# this includes: sentence segmentation and word tokenization:\n",
    "wikitext2 = PlaintextCorpusReader(\n",
    "    'wikitext-2-raw',\n",
    "    ['wiki.train.raw', 'wiki.valid.raw', 'wiki.test.raw'],\n",
    ")\n",
    "word_tokenizer = wikitext2._word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split:\n",
    "train = wikitext2.sents('wiki.train.raw')\n",
    "test = wikitext2.sents('wiki.test.raw')\n",
    "\n",
    "# the vocabulary based on the training data:\n",
    "vocab = nltk.lm.Vocabulary([\n",
    "    word\n",
    "    for sent in train\n",
    "    for word in sent\n",
    "], unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n-grams\n",
    "def build_ngrams(sent, n):\n",
    "    # pad both ends for corner-ngrams:\n",
    "    sent = ['<s>']*(n-1) + sent + ['</s>']*(n-1)\n",
    "    # build the ngrams:\n",
    "    return list(ngrams(sent, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_tokinized:\n",
      "['Minecraft', 'is', 'a', 'sandbox', 'video', 'game', 'developed', 'by', 'Mojang', '.']\n",
      "sample_trigrams\n",
      "[('<s>', '<s>', 'Minecraft'), ('<s>', 'Minecraft', 'is'), ('Minecraft', 'is', 'a'), ('is', 'a', 'sandbox'), ('a', 'sandbox', 'video'), ('sandbox', 'video', 'game'), ('video', 'game', 'developed'), ('game', 'developed', 'by'), ('developed', 'by', 'Mojang'), ('by', 'Mojang', '.'), ('Mojang', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# run this cell to inspect how it works:\n",
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_tokinized = word_tokenizer.tokenize(sample)\n",
    "sample_trigrams = build_ngrams(sample_tokinized, n=3)\n",
    "print('sample_tokinized:')\n",
    "print(sample_tokinized)\n",
    "print('sample_trigrams')\n",
    "print(sample_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plain: 100%|██████████| 3/3 [01:04<00:00, 21.53s/it]\n",
      "smoothing: 100%|██████████| 3/3 [01:07<00:00, 22.44s/it]\n",
      "smoothing+interpolation: 100%|██████████| 3/3 [01:13<00:00, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 23s, sys: 1.56 s, total: 3min 24s\n",
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compare these two models:\n",
    "models = {\n",
    "    'plain': nltk.lm.MLE, # plain count-based ngrams\n",
    "    'smoothing': nltk.lm.Laplace, # with laplace smoothing\n",
    "    'smoothing+interpolation': nltk.lm.KneserNeyInterpolated, # Modified Kneser & Ney \n",
    "}\n",
    "\n",
    "for lm_name in models:\n",
    "    # build and train the language model:\n",
    "    models[lm_name] = models[lm_name](_N, vocabulary=vocab)\n",
    "\n",
    "    # train on all n-grams (equal or lower order): N, N-1, ..., 1.\n",
    "    for n in tqdm(range(_N, 0, -1), desc=lm_name):\n",
    "        models[lm_name].fit([build_ngrams(sent, n) for sent in train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'the': 113161, ',': 99925, '.': 78888, 'of': 56889, 'and': 50605, 'in': 39488, 'to': 39190, 'a': 34269, '=': 29570, '\"': 28309, ...}),\n",
       " <ConditionalFreqDist with 75988 conditions>,\n",
       " <ConditionalFreqDist with 701600 conditions>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand how fit words:\n",
    "# fit() method builds all kinds of count dictionaries:\n",
    "(\n",
    "    models['plain'].counts[1], # unigrams\n",
    "    models['plain'].counts[2], # bi-grams for conditional count freq (w_{t} | w_{t-1})\n",
    "    models['plain'].counts[3], # tri-grams for conditional count freq (w_{t} | w_{t-2} w_{t-1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A', 'large'),\n",
       " FreqDist({'number': 4, 'variety': 3, 'portion': 3, 'team': 1, 'tent': 1, 'oil': 1, 'pyramid': 1, 'camp': 1, 'rear': 1, 'network': 1, ...}))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example: \n",
    "# Count( word_3='numer'   | word_1 = 'A', word_2 ='large' ) = 4\n",
    "# Count( word_3='variety' | word_1 = 'A', word_2 ='large' ) = 3\n",
    "# ...\n",
    "list(models['plain'].counts[3].items())[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand this:\n",
    "models['plain'].counts[3][('A', 'large')]['number'] / sum(models['plain'].counts[3][('A', 'large')].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['plain'].score('number', ('A', 'large'))\n",
    "# more details in chapter 3 equation 3.12.\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'very', 'partisan', 'assembly', ',', 'and', 'they', 'did', '.', '<UNK>']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the plain model for random language generation:\n",
    "models['plain'].generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.527499220336294"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect log probabilities:\n",
    "models['plain'].logscore('mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain model:\n",
      "1-gram inf\n",
      "2-gram inf\n",
      "3-gram inf\n",
      "\n",
      "smoothing model:\n",
      "1-gram 5089.599724571091\n",
      "2-gram 4218.755058726706\n",
      "3-gram 13170.588563844552\n",
      "\n",
      "smoothing+interpolation model:\n",
      "1-gram 75987.99999999977\n",
      "2-gram 3210.0833649081874\n",
      "3-gram inf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\")\n",
    "    for n in range(1, _N+1):\n",
    "        print(f\"{n}-gram\", models[model_name].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why these models have `<UNK>` token? What is the log-probability of <UNK> in three models? (3pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why plain count-based MLE model fails to produce perplexities? What are the possible solutions for it? (3pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Show with an example why Laplace smoothing can produce perplexity for unseen words? (3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English and python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why perplexity of bi-grams are lower than unigrams? (3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anwer here (in English and python)\n",
    "# use models['smoothing'].counts[2] to show how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional 1: measure perplexity of conditional trigrams (10pt)\n",
    "\n",
    "The neural network below is based on Bengio et al. (2003). It is trained on moving windows described in chapter 9 figure 9.1 but with trigrams instead of 4-grams.\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "You don't need to train the model. However, a stand alone python code is provided in `bengio_lm.py` if you want to try training it on GPU.\n",
    "\n",
    "Read the code below then report the perplexity of the language model on the sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # neural network framework\n",
    "\n",
    "# encoding the tokens:\n",
    "vocab_list = [word for word, freq in vocab.counts.most_common() if freq > 1]\n",
    "word2idx = {word: idx for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "idx2word = {idx: word for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "\n",
    "def token_encoder(tokens):\n",
    "    if type(tokens) in {list, tuple}:\n",
    "        return [word2idx[token] if token in word2idx else word2idx[vocab.unk_label] for token in tokens]\n",
    "    elif type(tokens) == str:\n",
    "        token = tokens\n",
    "        return word2idx[token] if token in word2idx else word2idx[vocab.unk_label]\n",
    "    print(type(tokens))\n",
    "\n",
    "# moving window language model:\n",
    "# https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "class BengioLM(torch.nn.Module):\n",
    "    def __init__(self, context_size=2, dim=50):\n",
    "        super(BengioLM, self).__init__()\n",
    "        # defining the parameters of the model\n",
    "        self.C = torch.nn.Embedding(len(word2idx), dim) # C\n",
    "        self.Hx_d = torch.nn.Linear(context_size*dim, dim) # d, H\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.Wx_Uf_b = torch.nn.Linear((context_size + 1) * dim, len(word2idx)) # b, U, W\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss_fn = torch.nn.NLLLoss() # negative-log-likelihood loss\n",
    "    \n",
    "    def forward(self, context, target_idx=None):\n",
    "        # function of the model\n",
    "        batch_size = context.shape[0]\n",
    "        x = self.C(context).view(batch_size,-1)\n",
    "        x = torch.cat([x, self.tanh(self.Hx_d(x))], dim=-1)\n",
    "        logprob = self.logsoftmax(self.Wx_Uf_b(x))\n",
    "        \n",
    "        if target_idx is None:\n",
    "            return logprob\n",
    "        else:\n",
    "            loss = self.loss_fn(logprob, target_idx)\n",
    "            return logprob, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is trained with Stochastic Gradient Descent with 10 epochs (skip this):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BengioLM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "sentence_batch_size = 32\n",
    "\n",
    "for e in range(10):\n",
    "    progress_bar = tqdm(range(0, len(train), sentence_batch_size))\n",
    "    for i in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts, targets = zip(*[\n",
    "            (token_encoder(tokens[:-1]), token_encoder(tokens[-1]))\n",
    "            for sent in train[i:i+sentence_batch_size]\n",
    "            for tokens in build_ngrams(sent, 3)\n",
    "        ])\n",
    "\n",
    "        logprob, loss = model.forward(torch.tensor(contexts), torch.tensor(targets))\n",
    "        progress_bar.set_description_str(f\"epoch={e+1},loss={loss.item():.3f}\")\n",
    "        progress_bar.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we ran the training code above on GPU and saved it in model.pt.\n",
    "# load the pre-trained language model:\n",
    "device = torch.device('cpu')\n",
    "model = BengioLM()\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minecraft tensor(-3.5543, grad_fn=<SelectBackward>)\n",
      "is tensor(-3.8908, grad_fn=<SelectBackward>)\n",
      "a tensor(-1.8586, grad_fn=<SelectBackward>)\n",
      "sandbox tensor(-3.5061, grad_fn=<SelectBackward>)\n",
      "video tensor(-8.3849, grad_fn=<SelectBackward>)\n",
      "game tensor(-2.7251, grad_fn=<SelectBackward>)\n",
      "developed tensor(-5.6833, grad_fn=<SelectBackward>)\n",
      "by tensor(-2.9111, grad_fn=<SelectBackward>)\n",
      "Mojang tensor(-2.7338, grad_fn=<SelectBackward>)\n",
      ". tensor(-3.0074, grad_fn=<SelectBackward>)\n",
      "</s> tensor(-0.0465, grad_fn=<SelectBackward>)\n",
      "</s> tensor(-1.6689e-06, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# this is how you can get the conditional log-probabilities of all words in the sentence\n",
    "# P(target | w0, w1):\n",
    "for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sample), n=3):\n",
    "    logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "    print(target, logprobs[0, token_encoder(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code here to report Perplexity of the sample sentence.\n",
    "\n",
    "For more information got to chapter 3, section 3.2.1 and chapter 9, equation 9.12.\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity of a sentence \n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional 2: implement a generate function using pre-trained language model above (10pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
