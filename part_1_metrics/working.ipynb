{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the code and implement the evaluaiton metrics.\n",
    "\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown corpus:\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# load a tagger models\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "# Naive Bayes MLE \n",
    "from nltk.tag.sequential import NgramTagger\n",
    "\n",
    "# tagset mapping:\n",
    "from nltk.tag.mapping import map_tag\n",
    "\n",
    "# plotting:\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# you can compare you implementation with these\n",
    "# evaluation metrics:\n",
    "from sklearn.metrics import (\n",
    "    f1_score as _f1_score, \n",
    "    precision_score as _precision_score, \n",
    "    recall_score as _recall_score,\n",
    "    accuracy_score as _accuracy_score\n",
    ")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing:\n",
    "test_train_split = 500\n",
    "test_set = brown.tagged_sents()[:test_train_split]\n",
    "train_set = brown.tagged_sents()[test_train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained perceptron tagger:\n",
    "perceptron_tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.3 s, sys: 2.06 s, total: 51.3 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train Naive Bayes / count-based ngram taggers:\n",
    "unigram_tagger = NgramTagger(1, train=train_set)\n",
    "bigram_tagger_nobackoff = NgramTagger(2, train=train_set)\n",
    "bigram_tagger = NgramTagger(2, train=train_set, backoff=unigram_tagger)\n",
    "trigram_tagger = NgramTagger(3, train=train_set, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Perceptron\": perceptron_tagger, \n",
    "    \"Unigram\": unigram_tagger, \n",
    "    \"Bigram\": bigram_tagger, \n",
    "    \"Trigram\": trigram_tagger, \n",
    "    \"Bigram-backoff\": bigram_tagger_nobackoff, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models\n",
    "\n",
    "The test dataset and the models are based on the English Penn TreeBank tagsets. However, we don't need that fine degree of granularity. Therefore, we map each tag onto unviversal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "# the ground truth labels according to the dataset:\n",
    "tags_true = [\n",
    "    map_tag(\"en-brown\", \"universal\", tag)\n",
    "    for tagged_sent in test_set\n",
    "    for word, tag in tagged_sent\n",
    "]\n",
    "\n",
    "# strip the tags:\n",
    "test_set_sents = [\n",
    "    [word for word, tag in tagged_sent]\n",
    "    for tagged_sent in test_set\n",
    "]\n",
    "\n",
    "tagset = sorted(list(set(tags_true)))\n",
    "print(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_preds = dict()\n",
    "for model_name, model in models.items():\n",
    "    tags_pred = [\n",
    "        map_tag(\"en-ptb\", \"universal\", tag) if model_name == \"Perceptron\" else map_tag(\"en-brown\", \"universal\", tag)\n",
    "        for sent in test_set_sents\n",
    "        for word, tag in model.tag(sent)\n",
    "    ]\n",
    "    models_preds[model_name] = tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Perceptron', 'Unigram', 'Bigram', 'Trigram', 'Bigram-backoff'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_pred = models_preds['Unigram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on formulas\n",
    "\n",
    "$precision = \\frac{\\text{tp}}{\\text{tp + fp}}$\n",
    "\n",
    "\n",
    "$recall = \\frac{\\text{tp}}{\\text{tp + fn}}$\n",
    "\n",
    "\n",
    "$accuracy = \\frac{\\text{tp + tn}}{\\text{tp + fp + fn + tn}}$\n",
    "\n",
    "$f_1 = \\frac{2PR}{P+R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = tags_true, tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = [\n",
    "    [0 for x in range(len(tagset))]\n",
    "    for x in range(len(tagset))\n",
    "]\n",
    "\n",
    "print(tagset)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset.index('ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for _ in tagset:\n",
    "    row = [0 for tag in tagset]\n",
    "    matrix.append(row)\n",
    "\n",
    "indices = {tagset: idx for idx, tagset in enumerate(tagset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset.index('ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for genre, doc in testing_data:\n",
    "#     this_guess = guess(model, doc)\n",
    "\n",
    "#     matrix[indices[tagset]][indices[this_guess]] += 1\n",
    "\n",
    "# return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    \"\"\"\n",
    "    number_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            number_correct += 1\n",
    "    return number_correct/(len(y_true))\n",
    "    \n",
    "def precision_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "        elif average == 'macro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "    else:\n",
    "        res = []\n",
    "        for tag in tagset:\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            for i in range(len(y_true)):\n",
    "                if y_pred[i] == tag:\n",
    "                    if y_true[i] == tag:\n",
    "                        tp += 1 \n",
    "                    else:\n",
    "                        fp += 1\n",
    "            res.append(tp / (tp + fp))\n",
    "        return res\n",
    "\n",
    "def recall_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "        elif average == 'macro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "    else:\n",
    "        res = []\n",
    "        for tag in tagset:\n",
    "            tp = 0\n",
    "            fn = 0\n",
    "            for i in range(len(y_true)):\n",
    "                if y_true[i] == tag:\n",
    "                    if y_pred[i] == tag:\n",
    "                        tp += 1 \n",
    "                    else:\n",
    "                        fn += 1\n",
    "            res.append(tp / (tp + fn))\n",
    "        return res\n",
    "\n",
    "def f1_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \"\"\"\n",
    "    # you can call recall_score and precision_score.\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "        elif average == 'macro':\n",
    "            #\n",
    "            # your code here\n",
    "            #\n",
    "            return # the score with micro averaging\n",
    "    else:\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        return # list of score of each label \n",
    "    \n",
    "\n",
    "def all_metrics(y_true, y_pred, labels=None, average=None):\n",
    "    # you can compare you implementation with these\n",
    "#     return (\n",
    "#         _precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _accuracy_score(y_true, y_pred)\n",
    "#     )\n",
    "    # remove the likes above and use the function calls below: \n",
    "    return (\n",
    "       _precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "       _recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "       _f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "       _accuracy_score(y_true, y_pred)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              |       |         macro       |         micro\n",
      "  model name  |  acc  | preci  recal    f1  | preci  recal    f1\n",
      "----------------------------------------------------------\n",
      "Perceptron    | 93.66 | 86.17  88.79  87.23 | 93.66  93.66  93.66\n",
      "Unigram       | 93.30 | 86.11  94.57  86.09 | 93.30  93.30  93.30\n",
      "Bigram        | 94.40 | 87.36  94.11  86.71 | 94.40  94.40  94.40\n",
      "Trigram       | 94.54 | 87.53  94.48  86.97 | 94.54  94.54  94.54\n",
      "Bigram-backoff| 24.63 | 87.72  31.96  36.74 | 24.63  24.63  24.63\n"
     ]
    }
   ],
   "source": [
    "models_preds = dict()\n",
    "print(\"              |       |         macro       |         micro\")\n",
    "print(\"  model name  |  acc  | preci  recal    f1  | preci  recal    f1\")\n",
    "print(\"-\"*58)\n",
    "for model_name, model in models.items():\n",
    "    tags_pred = [\n",
    "        map_tag(\"en-ptb\", \"universal\", tag) if model_name == \"Perceptron\" else map_tag(\"en-brown\", \"universal\", tag)\n",
    "        for sent in test_set_sents\n",
    "        for word, tag in model.tag(sent)\n",
    "    ]\n",
    "    models_preds[model_name] = tags_pred\n",
    "    # print the results\n",
    "    precision_macro, recall_macro, f1score_macro, accuracy = all_metrics(tags_true, tags_pred, average='macro')\n",
    "    precision_micro, recall_micro, f1score_micro, _ = all_metrics(tags_true, tags_pred, average='micro')\n",
    "    print(f\"{model_name:14}| {100*accuracy:5.2f} | {100*precision_macro:5.2f}  {100*recall_macro:5.2f}  {100*f1score_macro:5.2f} | {100*precision_micro:5.2f}  {100*recall_micro:5.2f}  {100*f1score_micro:5.2f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(y_true, y_pred, labels=tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = recall_score(y_true, y_pred, labels=tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(prec)\n",
    "r = np.array(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = p * r * 2 / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8608748366962331"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Perceptron\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      ".\t   100.00\t 99.76\t   99.88\n",
      "ADJ\t    83.18\t 80.25\t   81.69\n",
      "ADP\t    98.26\t 90.60\t   94.28\n",
      "ADV\t    89.88\t 89.88\t   89.88\n",
      "CONJ\t    99.61\t 99.61\t   99.61\n",
      "DET\t    95.98\t 92.34\t   94.12\n",
      "NOUN\t    94.26\t 97.28\t   95.74\n",
      "NUM\t    89.18\t 98.10\t   93.42\n",
      "PRON\t    72.41\t 97.15\t   82.98\n",
      "PRT\t    63.31\t 77.26\t   69.59\n",
      "VERB\t    98.00\t 93.24\t   95.56\n",
      "X\t    50.00\t 50.00\t   50.00\n",
      "==================================================\n",
      "==================================================\n",
      "Unigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      ".\t   100.00\t100.00\t  100.00\n",
      "ADJ\t    90.91\t 86.79\t   88.80\n",
      "ADP\t    97.30\t 91.13\t   94.11\n",
      "ADV\t    84.68\t 87.20\t   85.92\n",
      "CONJ\t    99.61\t100.00\t   99.80\n",
      "DET\t    99.78\t 99.56\t   99.67\n",
      "NOUN\t    96.41\t 90.59\t   93.41\n",
      "NUM\t    99.49\t 92.86\t   96.06\n",
      "PRON\t    99.64\t 97.15\t   98.38\n",
      "PRT\t    67.77\t 96.39\t   79.58\n",
      "VERB\t    96.45\t 93.14\t   94.76\n",
      "X\t     1.29\t100.00\t    2.54\n",
      "==================================================\n",
      "==================================================\n",
      "Bigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      ".\t   100.00\t100.00\t  100.00\n",
      "ADJ\t    93.51\t 90.57\t   92.01\n",
      "ADP\t    94.72\t 94.47\t   94.59\n",
      "ADV\t    91.24\t 89.88\t   90.55\n",
      "CONJ\t    99.22\t 99.61\t   99.41\n",
      "DET\t    99.71\t 99.78\t   99.74\n",
      "NOUN\t    97.79\t 91.80\t   94.70\n",
      "NUM\t    99.49\t 92.86\t   96.06\n",
      "PRON\t    97.16\t 97.51\t   97.34\n",
      "PRT\t    76.16\t 77.26\t   76.70\n",
      "VERB\t    98.05\t 95.62\t   96.82\n",
      "X\t     1.29\t100.00\t    2.55\n",
      "==================================================\n",
      "==================================================\n",
      "Trigram\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      ".\t   100.00\t100.00\t  100.00\n",
      "ADJ\t    93.73\t 90.31\t   91.99\n",
      "ADP\t    95.28\t 94.27\t   94.77\n",
      "ADV\t    91.82\t 90.18\t   90.99\n",
      "CONJ\t    99.61\t 99.61\t   99.61\n",
      "DET\t    99.56\t 99.71\t   99.64\n",
      "NOUN\t    97.94\t 91.94\t   94.84\n",
      "NUM\t    99.49\t 92.86\t   96.06\n",
      "PRON\t    96.18\t 98.58\t   97.36\n",
      "PRT\t    77.43\t 80.51\t   78.94\n",
      "VERB\t    98.06\t 95.83\t   96.93\n",
      "X\t     1.29\t100.00\t    2.55\n",
      "==================================================\n",
      "==================================================\n",
      "Bigram-backoff\n",
      "\n",
      "tag\tprecision\trecall\tf1-score\n",
      "--------------------------------------------------\n",
      ".\t   100.00\t 21.07\t   34.81\n",
      "ADJ\t    93.96\t 21.51\t   35.01\n",
      "ADP\t    95.39\t 22.07\t   35.84\n",
      "ADV\t    93.86\t 31.85\t   47.56\n",
      "CONJ\t   100.00\t 16.54\t   28.38\n",
      "DET\t    99.79\t 35.04\t   51.86\n",
      "NOUN\t    98.43\t 21.06\t   34.70\n",
      "NUM\t   100.00\t 23.33\t   37.84\n",
      "PRON\t    98.40\t 43.77\t   60.59\n",
      "PRT\t    73.68\t 20.22\t   31.73\n",
      "VERB\t    99.03\t 27.03\t   42.47\n",
      "X\t     0.05\t100.00\t    0.09\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for model_name, tags_pred in models_preds.items():\n",
    "    print('='*50)\n",
    "    print(model_name)\n",
    "    print('')\n",
    "    precisions, recalls, f1scores, _ = all_metrics(tags_true, tags_pred, labels=tagset)\n",
    "    print(\"tag\\tprecision\\trecall\\tf1-score\")\n",
    "    print(\"-\"*50)\n",
    "    for tag, precision, recall, f1score in zip(tagset, precisions, recalls, f1scores):\n",
    "        print(f\"{tag}\\t{100*precision:9.2f}\\t{100*recall:6.2f}\\t{100*f1score:8.2f}\")\n",
    "    print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, tags_pred in models_preds.items():\n",
    "    print('='*50)\n",
    "    print(model_name)\n",
    "    print('')\n",
    "    precisions, recalls, f1scores, _ = all_metrics(tags_true, tags_pred, labels=tagset)\n",
    "    print(\"tag\\tprecision\\trecall\\tf1-score\")\n",
    "    print(\"-\"*50)\n",
    "    for tag, precision, recall, f1score in zip(tagset, precisions, recalls, f1scores):\n",
    "        print(f\"{tag}\\t{100*precision:9.2f}\\t{100*recall:6.2f}\\t{100*f1score:8.2f}\")\n",
    "    print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
