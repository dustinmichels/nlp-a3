{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the code and implement the evaluaiton metrics.\n",
    "\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown corpus:\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# load a tagger models\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "# Naive Bayes MLE \n",
    "from nltk.tag.sequential import NgramTagger\n",
    "\n",
    "# tagset mapping:\n",
    "from nltk.tag.mapping import map_tag\n",
    "\n",
    "# plotting:\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# you can compare you implementation with these\n",
    "# evaluation metrics:\n",
    "from sklearn.metrics import (\n",
    "    f1_score as _f1_score, \n",
    "    precision_score as _precision_score, \n",
    "    recall_score as _recall_score,\n",
    "    accuracy_score as _accuracy_score\n",
    ")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing:\n",
    "test_train_split = 500\n",
    "test_set = brown.tagged_sents()[:test_train_split]\n",
    "train_set = brown.tagged_sents()[test_train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained perceptron tagger:\n",
    "perceptron_tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train Naive Bayes / count-based ngram taggers:\n",
    "unigram_tagger = NgramTagger(1, train=train_set)\n",
    "bigram_tagger_nobackoff = NgramTagger(2, train=train_set)\n",
    "bigram_tagger = NgramTagger(2, train=train_set, backoff=unigram_tagger)\n",
    "trigram_tagger = NgramTagger(3, train=train_set, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Perceptron\": perceptron_tagger, \n",
    "    \"Unigram\": unigram_tagger, \n",
    "    \"Bigram\": bigram_tagger, \n",
    "    \"Trigram\": trigram_tagger, \n",
    "    \"Bigram-backoff\": bigram_tagger_nobackoff, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models\n",
    "\n",
    "The test dataset and the models are based on the English Penn TreeBank tagsets. However, we don't need that fine degree of granularity. Therefore, we map each tag onto unviversal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ground truth labels according to the dataset:\n",
    "tags_true = [\n",
    "    map_tag(\"en-brown\", \"universal\", tag)\n",
    "    for tagged_sent in test_set\n",
    "    for word, tag in tagged_sent\n",
    "]\n",
    "\n",
    "# strip the tags:\n",
    "test_set_sents = [\n",
    "    [word for word, tag in tagged_sent]\n",
    "    for tagged_sent in test_set\n",
    "]\n",
    "\n",
    "tagset = sorted(list(set(tags_true)))\n",
    "print(tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$precision = \\frac{\\text{tp}}{\\text{tp + fp}}$\n",
    "\n",
    "\n",
    "$recall = \\frac{\\text{tp}}{\\text{tp + fn}}$\n",
    "\n",
    "\n",
    "$accuracy = \\frac{\\text{tp + tn}}{\\text{tp + fp + fn + tn}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    \"\"\"\n",
    "    number_correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            number_correct += 1\n",
    "    return number_correct/(len(y_true))\n",
    "    \n",
    "def precision_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "    def compute_precisions():\n",
    "        precisions = []\n",
    "        tps = []\n",
    "        fps = []\n",
    "        for tag in tagset:\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            for i in range(len(y_true)):\n",
    "                if y_pred[i] == tag:\n",
    "                    if y_true[i] == tag:\n",
    "                        tp += 1 \n",
    "                    else:\n",
    "                        fp += 1\n",
    "            precisions.append(tp / (tp + fp))\n",
    "            tps.append(tp)\n",
    "            fps.append(fp)\n",
    "        return precisions, tps, fps \n",
    "    \n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            precisions, tps, fps = compute_precisions()\n",
    "            return sum(tps) / (sum(tps) + sum(fps))\n",
    "        elif average == 'macro':\n",
    "            precisions, tps, fps = compute_precisions()\n",
    "            return sum(precisions) / len(precisions)\n",
    "    else:\n",
    "        precisions, tps, fps = compute_precisions()\n",
    "        return precisions\n",
    "\n",
    "def recall_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \n",
    "    When true positive + false positive == 0 returns 0\n",
    "    \"\"\"\n",
    "    def compute_recalls():\n",
    "        recalls = []\n",
    "        tps = []\n",
    "        fns = []\n",
    "        for tag in tagset:\n",
    "            tp = 0\n",
    "            fn = 0\n",
    "            for i in range(len(y_true)):\n",
    "                if y_true[i] == tag:\n",
    "                    if y_pred[i] == tag:\n",
    "                        tp += 1 \n",
    "                    else:\n",
    "                        fn += 1\n",
    "            recalls.append(tp / (tp + fn))\n",
    "            tps.append(tp)\n",
    "            fns.append(fn)\n",
    "        return (recalls, tps, fns)\n",
    "    \n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            # tp / tp + fn\n",
    "            recalls, tps, fns = compute_recalls()\n",
    "            return sum(tps) / (sum(tps) + sum(fns))\n",
    "        elif average == 'macro':\n",
    "            recalls, tps, fns = compute_recalls()\n",
    "            return sum(recalls) / len(recalls)\n",
    "    else:\n",
    "        recalls, tps, fns = compute_recalls()\n",
    "        return recalls\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, labels=None, average=None):\n",
    "    \"\"\"\n",
    "    y_true : 1d array-like Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like Estimated targets as returned by a classifier.\n",
    "    labels : list of unique labels for sort out the result\n",
    "    average : string, ['micro', 'macro'] instead of defining labels.\n",
    "    \"\"\"\n",
    "    # you can call recall_score and precision_score.\n",
    "    if labels is None:\n",
    "        if average == 'micro':\n",
    "            p = precision_score(y_true, y_pred, average=average)\n",
    "            r = recall_score(y_true, y_pred, average=average)\n",
    "            return (2 * p * r) / (p + r)\n",
    "        elif average == 'macro':\n",
    "            \n",
    "            \n",
    "            p = precision_score(y_true, y_pred, average=average)\n",
    "            r = recall_score(y_true, y_pred, average=average)\n",
    "            return (2 * p * r) / (p + r)\n",
    "    else:\n",
    "        precision = precision_score(y_true, y_pred, labels=labels)\n",
    "        recall = recall_score(y_true, y_pred, labels=labels)\n",
    "        # 2pr / p + r\n",
    "        return (\n",
    "            [(p * r * 2) / (p + r) for p, r in zip(precision, recall)]\n",
    "        )\n",
    "        \n",
    "\n",
    "def all_metrics(y_true, y_pred, labels=None, average=None):\n",
    "    # you can compare you implementation with these\n",
    "#     return (\n",
    "#         _precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "#         _accuracy_score(y_true, y_pred)\n",
    "#     )\n",
    "    # remove the likes above and use the function calls below: \n",
    "    return (\n",
    "       precision_score(y_true, y_pred, labels=labels, average=average),\n",
    "       recall_score(y_true, y_pred, labels=labels, average=average),\n",
    "       f1_score(y_true, y_pred, labels=labels, average=average),\n",
    "       accuracy_score(y_true, y_pred)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_preds = dict()\n",
    "print(\"              |       |         macro       |         micro\")\n",
    "print(\"  model name  |  acc  | preci  recal    f1  | preci  recal    f1\")\n",
    "print(\"-\"*58)\n",
    "for model_name, model in models.items():\n",
    "    tags_pred = [\n",
    "        map_tag(\"en-ptb\", \"universal\", tag) if model_name == \"Perceptron\" else map_tag(\"en-brown\", \"universal\", tag)\n",
    "        for sent in test_set_sents\n",
    "        for word, tag in model.tag(sent)\n",
    "    ]\n",
    "    models_preds[model_name] = tags_pred\n",
    "    # print the results\n",
    "    precision_macro, recall_macro, f1score_macro, accuracy = all_metrics(tags_true, tags_pred, average='macro')\n",
    "    precision_micro, recall_micro, f1score_micro, _ = all_metrics(tags_true, tags_pred, average='micro')\n",
    "    print(f\"{model_name:14}| {100*accuracy:5.2f} | {100*precision_macro:5.2f}  {100*recall_macro:5.2f}  {100*f1score_macro:5.2f} | {100*precision_micro:5.2f}  {100*recall_micro:5.2f}  {100*f1score_micro:5.2f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, tags_pred in models_preds.items():\n",
    "    print('='*50)\n",
    "    print(model_name)\n",
    "    print('')\n",
    "    precisions, recalls, f1scores, _ = all_metrics(tags_true, tags_pred, labels=tagset)\n",
    "    print(\"tag\\tprecision\\trecall\\tf1-score\")\n",
    "    print(\"-\"*50)\n",
    "    for tag, precision, recall, f1score in zip(tagset, precisions, recalls, f1scores):\n",
    "        print(f\"{tag}\\t{100*precision:9.2f}\\t{100*recall:6.2f}\\t{100*f1score:8.2f}\")\n",
    "    print('='*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
