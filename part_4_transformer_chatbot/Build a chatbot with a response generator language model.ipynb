{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Language Model For Language Generation (17 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to understand input and output of a large scale pre-trained generative language model. \n",
    "Then, use it as an encoder-decoder chatbot described on Chapter 26, Section 26.2.2 (Jurafsky and Martin, 2019).\n",
    "https://web.stanford.edu/~jurafsky/slp3/26.pdf\n",
    "\n",
    "We will test DialoGPT, a large scale generative language model, pre-trained on conversational responces. \n",
    "For extended readings the demo paper describes the model here:\n",
    "https://www.aclweb.org/anthology/2020.acl-demos.30/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0 available.\n",
      "TensorFlow version 2.3.1 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# this will download the pre-trained language model:\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = GPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 has 50,257 sub-word units for tokenization\n"
     ]
    }
   ],
   "source": [
    "# The tokenization is based on a model called Byte-Pair Encoding (BPE)\n",
    "# This method avoid any out-of-vocabulary (OOV) situations.\n",
    "print(f\"GPT2 has {tokenizer.vocab_size:,} sub-word units for tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input tensor:\n",
      "torch.Size([1, 9])\n",
      "Token ids: tensor([[4342,  318,  257, 2420,  284, 2198,  503,  262, 5128]])\n",
      "None of tokens are masked: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Each token is translated into an index number in vocabulary.\n",
    "inputs = tokenizer(\"Here is a text to check out the input\", return_tensors=\"pt\", )\n",
    "# 'return_tensors=\"pt\"' means the type of output must be PyTorch Tensor.\n",
    "\n",
    "# Model only accepts batches of inputs.\n",
    "print(\"The shape of the input tensor:\")\n",
    "print(inputs['input_ids'].shape)\n",
    "# the first dimension is the number of instances in the batch, \n",
    "# the second number is the number of tokens in each instance.\n",
    "\n",
    "print(\"Token ids:\", inputs['input_ids'])\n",
    "# We also need to specify if any token is masked:\n",
    "print(\"None of tokens are masked:\", inputs['attention_mask']) # 1: not masked, 0: masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'Ġis', 'Ġa', 'Ġtext', 'Ġto', 'Ġcheck', 'Ġout', 'Ġthe', 'Ġinput']\n",
      "Here is a text to check out the input\n"
     ]
    }
   ],
   "source": [
    "# the tokens \n",
    "print([tokenizer.decoder[idx] for idx in inputs['input_ids'][0].tolist()])\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forwar method in the model, produces logits (scores before Softmax) of the next word\n",
    "outputs = model.forward(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    ")\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6597, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# If you pass \"labels\", then it produces the negative log-likelihoos loss of the predictions too:\n",
    "outputs = model.forward(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    labels=inputs['input_ids'], # outputs\n",
    ")\n",
    "logits = outputs.logits\n",
    "loss = outputs.loss\n",
    "\n",
    "# the gradient of loss with respect to each parameter was used for training the model.\n",
    "# loss.grad_fn keeps the gradient function.\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model for generation\n",
    "\n",
    "Starting from one sentence, one can use the model to generate the most likely tokens according to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a good day?<|endoftext|>I'm not sure what you're trying to say.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# if you start from a sentence bellow:\n",
    "input_utterance = \"What a good day?\"\n",
    "generated = tokenizer.encode(input_utterance + tokenizer.eos_token)\n",
    "context = torch.tensor([generated])\n",
    "past = None\n",
    "\n",
    "for i in range(100):\n",
    "    output = model.forward(context, past_key_values=past)\n",
    "    past = output.past_key_values\n",
    "    logits = output.logits\n",
    "    \n",
    "    # choose the most likely next token:\n",
    "    token = torch.argmax(logits[..., -1, :])\n",
    "    \n",
    "    # add it to the generated sentence\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)\n",
    "    \n",
    "    # stop, if the generated token is the end token! \n",
    "    if token == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "sequence = tokenizer.decode(generated)\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam search for better output \n",
    "\n",
    "Instead of the greedy algorithm implemented above, *beam search* could be used to find a more likely sequence. The generation algorithm with `num_beams=1` is equivalant to the greedy algorithm. Larger number of beam takes more time for search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: What a good day?\n",
      ">> Bot: What a great day!\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_utterance + tokenizer.eos_token, return_tensors='pt') \n",
    "\n",
    "# generate text until either reaches the end token, or the number of tokens reaches max_length.\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_beams=4,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# you can structure input/output:\n",
    "print(\">> User:\", input_utterance)\n",
    "print(\">> Bot:\", tokenizer.decode(output[0, input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, increasing the beam size and finding a more likely sequence is not going to produce a better sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: Have you seen my cat?\n",
      ">> Bot1 (num_beam=1): I have. He's a good boy.\n",
      ">> Bot2 (num_beam=2): No, but I have seen your cat.\n",
      ">> Bot2 (num_beam=3): Have you seen my cat?\n",
      ">> Bot2 (num_beam=4): Have you seen my cat?\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Have you seen my cat?\" + tokenizer.eos_token, return_tensors='pt') \n",
    "output1 = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_beams=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# you can structure input/output:\n",
    "print(\">> User:\", \"Have you seen my cat?\")\n",
    "print(\">> Bot1 (num_beam=1):\", tokenizer.decode(output1[0, input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "output2 = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_beams=2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(\">> Bot2 (num_beam=2):\", tokenizer.decode(output2[0, input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "output2 = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_beams=3,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(\">> Bot2 (num_beam=3):\", tokenizer.decode(output2[0, input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "output2 = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_beams=4,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\">> Bot2 (num_beam=4):\", tokenizer.decode(output2[0, input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a history of interactions to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Bot: What a lovely day!\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    tokenizer.encode(\"Hello!\" + tokenizer.eos_token, return_tensors='pt'), # user\n",
    "    tokenizer.encode(\"Hi!\" + tokenizer.eos_token, return_tensors='pt'), # bot\n",
    "    tokenizer.encode(\"What a good day!\" + tokenizer.eos_token, return_tensors='pt'), # user\n",
    "]\n",
    "\n",
    "bot_input_ids = torch.cat(history, dim=-1)\n",
    "bot_output_ids = model.generate(bot_input_ids, max_length=1000, num_beams=1, pad_token_id=tokenizer.eos_token_id)\n",
    "last_output_ids = bot_output_ids[:, bot_input_ids.shape[-1]:]\n",
    "\n",
    "print(\">> Bot: {}\".format(tokenizer.decode(last_output_ids[0], skip_special_tokens=True)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement it similar to Eliza in NLTK\n",
    "\n",
    "Eliza is a rule-based chatbot described in Chapter 26, section 26.2.1. J&M (2019).\n",
    "In Eliza, each user input is matched with a regular expression. If there is no specific pattern for an input it matches with `(.*)` which there are limited number of responces for it:\n",
    "\n",
    "```\n",
    "r\"(.*)\",\n",
    "(\n",
    "    \"Please tell me more.\",\n",
    "    \"Let's change focus a bit... Tell me about your family.\",\n",
    "    \"Can you elaborate on that?\",\n",
    "    \"Why do you say that %1?\",\n",
    "    \"I see.\",\n",
    "    \"Very interesting.\",\n",
    "    \"%1.\",\n",
    "    \"I see.  And what does that tell you?\",\n",
    "    \"How does that make you feel?\",\n",
    "    \"How do you feel when you say that?\",\n",
    ")\n",
    "```\n",
    "Source here: https://www.nltk.org/_modules/nltk/chat/eliza.html\n",
    "\n",
    "Modify the code below to use DialoGPT instead of regular expression pattern matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chat.util import Chat\n",
    "\n",
    "class DialoGPTChatbot(Chat):\n",
    "    def __init__(self):\n",
    "        # we don't need the pattern matching pairs. \n",
    "        # however, it is useful to have some rules about finishing the conversation.\n",
    "        # the quit responces are from Eliza chatbot. \n",
    "        super().__init__([(\n",
    "            r\"quit\",\n",
    "            (\n",
    "                \"Thank you for talking with me.\",\n",
    "                \"Good-bye.\",\n",
    "                \"Thank you, that will be $150.  Have a good day!\",\n",
    "            ),\n",
    "        ),], {})\n",
    "\n",
    "    def respond(self, str):\n",
    "        # regular expression pattern recognition\n",
    "        resp_org = super().respond(str)\n",
    "        \n",
    "        if resp_org is None:\n",
    "            #\n",
    "            # code here to generate response from DialoGPT\n",
    "            #\n",
    "            resp = \"text generated by DialoGPT\"\n",
    "        else:\n",
    "            resp = resp_org\n",
    "        return resp\n",
    "    \n",
    "    # Hold a conversation with a chatbot\n",
    "    def converse(self, quit=\"quit\"):\n",
    "        # change the code below to if you want to keep few more step of chat history:\n",
    "        user_input = \"\"\n",
    "        while user_input != quit:\n",
    "            user_input = quit\n",
    "            try:\n",
    "                user_input = input(\">\")\n",
    "            except EOFError:\n",
    "                print(user_input)\n",
    "            if user_input:\n",
    "                while user_input[-1] in \"!.\":\n",
    "                    user_input = user_input[:-1]\n",
    "                print(self.respond(user_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg =DialoGPTChatbot()\n",
    "dg.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to wrap this in a stand alone python (instead of Notebook) to be able to run on terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
