{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representations and Lexical Similarities (12 + 10 pt)\n",
    "\n",
    "For more reading on vector semantics got to Chapter 6, sections 6.4 and 6.8:\n",
    "https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "For wordnet exploration use this manual: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For additional wordnet discussions go to chapter 19: https://web.stanford.edu/~jurafsky/slp3/19.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# load word-vector glov\n",
    "import gensim.downloader as gensim_api\n",
    "glove_model = gensim_api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "from itertools import combinations, product\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['car', 'dog', 'banana', 'delicious', 'baguette', 'jumping', 'hugging', 'election']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Representations in English WordNet (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word above print their synsets\n",
    "# for each synset print all lemmas, hypernyms, hyponyms\n",
    "\n",
    "# \n",
    "# Write your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure The Lexical Similarity (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.000\n",
      "car       banana     0.000\n",
      "car       delicious  0.000\n",
      "car       baguette   0.000\n",
      "car       jumping    0.000\n",
      "car       hugging    0.000\n",
      "car       election   0.000\n",
      "dog       banana     0.000\n",
      "dog       delicious  0.000\n",
      "dog       baguette   0.000\n",
      "dog       jumping    0.000\n",
      "dog       hugging    0.000\n",
      "dog       election   0.000\n",
      "banana    delicious  0.000\n",
      "banana    baguette   0.000\n",
      "banana    jumping    0.000\n",
      "banana    hugging    0.000\n",
      "banana    election   0.000\n",
      "delicious baguette   0.000\n",
      "delicious jumping    0.000\n",
      "delicious hugging    0.000\n",
      "delicious election   0.000\n",
      "baguette  jumping    0.000\n",
      "baguette  hugging    0.000\n",
      "baguette  election   0.000\n",
      "jumping   hugging    0.000\n",
      "jumping   election   0.000\n",
      "hugging   election   0.000\n"
     ]
    }
   ],
   "source": [
    "# Wu-Palmer Similarity is a measure of similarity between to sense based on their depth distance. \n",
    "#\n",
    "# For each pair of words, find their closes sense based on Wu-Palmer Similarity.\n",
    "# List all word pairs and their highest possible wup_similarity. \n",
    "# Use wn.wup_similarity(s1, s2) and itertools (combinations and product).\n",
    "# if there is no connection between two words, put 0.\n",
    "\n",
    "wn_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    ### Your code here ###\n",
    "    max_sim = 0\n",
    "    ######################\n",
    "    wn_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "\n",
    "# which word pair are the most similar words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the similarities on GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.464\n",
      "car       banana     0.219\n",
      "car       delicious  0.068\n",
      "car       baguette   0.046\n",
      "car       jumping    0.516\n",
      "car       hugging    0.278\n",
      "car       election   0.333\n",
      "dog       banana     0.333\n",
      "dog       delicious  0.404\n",
      "dog       baguette   0.018\n",
      "dog       jumping    0.539\n",
      "dog       hugging    0.410\n",
      "dog       election   0.181\n",
      "banana    delicious  0.487\n",
      "banana    baguette   0.450\n",
      "banana    jumping    0.108\n",
      "banana    hugging    0.127\n",
      "banana    election   0.164\n",
      "delicious baguette   0.421\n",
      "delicious jumping    0.042\n",
      "delicious hugging    0.142\n",
      "delicious election   0.028\n",
      "baguette  jumping   -0.075\n",
      "baguette  hugging    0.161\n",
      "baguette  election  -0.091\n",
      "jumping   hugging    0.447\n",
      "jumping   election   0.206\n",
      "hugging   election  -0.076\n"
     ]
    }
   ],
   "source": [
    "glov_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = glove_model.similarity(word1, word2)\n",
    "    glov_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if two measures correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SpearmanrResult(correlation=nan, pvalue=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehdi/.pyenv/versions/3.7.6/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/mehdi/.pyenv/versions/3.7.6/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/mehdi/.pyenv/versions/3.7.6/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/Users/mehdi/.pyenv/versions/3.7.6/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/Users/mehdi/.pyenv/versions/3.7.6/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(glov_sims, wn_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Representations in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog = [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Each word is represented as a vector:\n",
    "print('dog =', glove_model['dog'])\n",
    "\n",
    "# matrix of all word vectors is trained as parameters of a language model:\n",
    "# P( target_word | context_word ) = f(word, context ; params)\n",
    "#\n",
    "# Words in a same sentence and in close proximity are in context of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cosine Similarity (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on equation 6.10 J&M (2019)\n",
    "# https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "#\n",
    "def cosine_sim(v1, v2):\n",
    "    out = 0\n",
    "    # code here\n",
    "    return out\n",
    "\n",
    "\n",
    "cosine_sim(glove_model['car'], glove_model['automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement top-n most similar words (+3pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in glove_model:\n",
    "def top_n(word, n):\n",
    "    # example: top_n('dog', 3) =  \n",
    "    #[('cat', 0.9218005537986755),\n",
    "    # ('dogs', 0.8513159155845642),\n",
    "    # ('horse', 0.7907583713531494)]\n",
    "    # similar to glove_model.most_similar('dog', topn=3)\n",
    "\n",
    "    out = []\n",
    "    #\n",
    "    # code here\n",
    "    # \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Examine Fairness In Data Driven Word Vectors (+10 pt)\n",
    "\n",
    "Caliskan et al. (2017) argues that word vectors learn human biases from data. \n",
    "\n",
    "Try to replicate one of the tests of the paper:\n",
    "\n",
    "Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science\n",
    "356.6334 (2017): 183-186. http://opus.bath.ac.uk/55288/\n",
    "\n",
    "\n",
    "For example on gender bias:\n",
    "- Male names: John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill.\n",
    "- Female names: Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna.\n",
    "- Career words : executive, management, professional, corporation, salary, office, business, career.\n",
    "- Family words : home, parents, children, family, cousins, marriage, wedding, relatives.\n",
    "\n",
    "\n",
    "Report the average cosine similarity of male names to career words, and compare it with the average similarity of female names to career words. (repeat for family words) \n",
    "\n",
    "tokens in GloVe model are all in lower case.\n",
    "\n",
    "Write at least one sentence to describe your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
